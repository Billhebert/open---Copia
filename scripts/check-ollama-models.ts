#!/usr/bin/env tsx
/**
 * Script para verificar modelos Ollama instalados
 */

import 'dotenv/config';

const OLLAMA_URL = process.env.OLLAMA_URL || 'http://localhost:11434';

async function checkOllamaModels() {
  console.log('='.repeat(60));
  console.log('üîç Verificando Modelos Ollama Instalados');
  console.log('='.repeat(60));
  console.log(`URL: ${OLLAMA_URL}\n`);

  try {
    const response = await fetch(`${OLLAMA_URL}/api/tags`, {
      signal: AbortSignal.timeout(5000),
    });

    if (!response.ok) {
      console.log('‚ùå Ollama n√£o est√° respondendo');
      console.log(`   Status: ${response.status}`);
      process.exit(1);
    }

    const data = await response.json();
    const models = data.models || [];

    if (models.length === 0) {
      console.log('‚ö†Ô∏è  Nenhum modelo instalado!');
      console.log('\nüì• Modelos recomendados para baixar:');
      console.log('   docker exec -it <container> ollama pull llama3.2:latest');
      console.log('   docker exec -it <container> ollama pull llama3.1:latest');
      console.log('   docker exec -it <container> ollama pull phi3:latest');
      process.exit(1);
    }

    console.log(`‚úÖ Encontrados ${models.length} modelos:\n`);

    for (const model of models) {
      const size = (model.size / 1024 / 1024 / 1024).toFixed(2);
      const modified = new Date(model.modified_at).toLocaleString('pt-BR');

      console.log(`üì¶ ${model.name}`);
      console.log(`   Tamanho: ${size} GB`);
      console.log(`   Modificado: ${modified}`);
      console.log(`   Digest: ${model.digest.substring(0, 20)}...`);
      console.log();
    }

    // Verifica modelos espec√≠ficos
    const hasLlama32 = models.some((m: any) => m.name.includes('llama3.2'));
    const hasLlama31 = models.some((m: any) => m.name.includes('llama3.1'));
    const hasLlama3 = models.some((m: any) => m.name.includes('llama3'));
    const hasPhi = models.some((m: any) => m.name.includes('phi'));

    console.log('='.repeat(60));
    console.log('üìä Modelos Dispon√≠veis para .env:');
    console.log('='.repeat(60));

    if (hasLlama32) {
      const model = models.find((m: any) => m.name.includes('llama3.2'));
      console.log(`‚úÖ OLLAMA_MODEL=${model.name}`);
    }
    if (hasLlama31) {
      const model = models.find((m: any) => m.name.includes('llama3.1'));
      console.log(`‚úÖ OLLAMA_MODEL=${model.name}`);
    }
    if (hasLlama3 && !hasLlama32 && !hasLlama31) {
      const model = models.find((m: any) => m.name.includes('llama3'));
      console.log(`‚úÖ OLLAMA_MODEL=${model.name}`);
    }
    if (hasPhi) {
      const model = models.find((m: any) => m.name.includes('phi'));
      console.log(`‚úÖ OLLAMA_MODEL=${model.name}`);
    }

    if (!hasLlama32 && !hasLlama31 && !hasLlama3 && !hasPhi) {
      console.log('‚ö†Ô∏è  Nenhum modelo de chat recomendado encontrado');
      console.log('\nüí° Use o primeiro modelo dispon√≠vel:');
      console.log(`   OLLAMA_MODEL=${models[0].name}`);
    }

    console.log('\n' + '='.repeat(60));

  } catch (error: any) {
    console.log('‚ùå Erro ao conectar com Ollama');
    console.log(`   Erro: ${error.message}`);
    console.log('\nüí° Verifique se o Docker est√° rodando:');
    console.log('   docker ps | grep ollama');
    console.log('\nüí° Ou inicie o Ollama:');
    console.log('   docker-compose up -d ollama');
    process.exit(1);
  }
}

checkOllamaModels().catch(console.error);
